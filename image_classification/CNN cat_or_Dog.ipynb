{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc5da36-5651-40b8-aad9-de39d830de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954512c-6dbd-4745-a128-27ec1c6f20b0",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "- they have given multiple images of cats and dogs\n",
    "- cat images are in one  folder and dog images are in one folder\n",
    "- since, we have 2 classes, its a binary image classification project\n",
    "- the given images are the different sizes\n",
    "\n",
    "\n",
    "# Data (Image)Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df43e034-a01c-4bdd-bfc4-53b49e585ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2506db8e-012a-4966-872a-dee976726573",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2)\n",
    "# maxscalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fed32ed-a406-4475-98a2-6d63cb887e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Resizing\n",
    "training_set = train_datagen.flow_from_directory(r\"C:\\Users\\SHRI\\OneDrive\\Desktop\\DataScience_DataFiles\\Image_data\\dataset\\training_set\",\n",
    "                                                target_size = (64,64),      # least quality of pixeel in dataset\n",
    "                                                 class_mode = 'binary'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b3ef09-2b64-4d4b-8e9b-783fd44c650b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices    # clAss names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5553b5d-0326-4a7f-96bd-68c0fc7b0c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2)\n",
    "# maxscalling\n",
    "\n",
    "#resizing\n",
    "test_set = test_datagen.flow_from_directory(r\"C:\\Users\\SHRI\\OneDrive\\Desktop\\DataScience_DataFiles\\Image_data\\dataset\\test_set\",\n",
    "                                                target_size = (64,64),      # least quality of pixeel in dataset\n",
    "                                                 class_mode = 'binary'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "207a1796-4b57-4c24-90f7-e42e4795d23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.class_indices    # clAss names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13beb9cc-b6b1-46d1-a834-6391ee535d54",
   "metadata": {},
   "source": [
    "# Modelling - Convolution Neural Network\n",
    "\n",
    "- initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "554bebd8-d398-4951-b67a-bae982c2b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "classifier = Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a31ff-4b18-4d99-860a-a65d83f9fc59",
   "metadata": {},
   "source": [
    "**step 1 - Convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6644c3a-5098-4e0c-9d80-e771ddb7344c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHRI\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "classifier.add(Conv2D(\n",
    "    input_shape=[64, 64, 3],   # Image dimensions: 64x64 with 3 color channels (RGB)\n",
    "    filters=32,                 # Number of filters\n",
    "    kernel_size=3,              # Size of each filter (3x3)\n",
    "    activation='relu'           # Correct spelling of the parameter\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01029272-3af2-4a53-91ed-cbdf97dd7ddc",
   "metadata": {},
   "source": [
    "**step 2 - Max Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc19f58-926c-49f7-b376-6b356ae5d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32afd65b-fac5-4417-9f54-0af2425cdf38",
   "metadata": {},
   "source": [
    "**Step 3 - Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103e53f2-7415-419c-943d-f3d3a857b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "classifier.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c762c-ea73-4806-9694-2277bf7ab71c",
   "metadata": {},
   "source": [
    "**Step 4 - Full Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "374b8d3b-9959-4ab6-a849-667f811fb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# 1st hiddent layer with 128 neurons\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "# output layer with 1 neurons\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3760744-fc69-4aea-9cd6-96ea8caa0544",
   "metadata": {},
   "source": [
    "**Training the CNN model with train data and Testing the model with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d64a3dae-1ca9-4075-b432-774161843c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47ce1d89-05d0-4fcf-b4ba-512a155c1e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHRI\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 647ms/step - accuracy: 0.5705 - loss: 0.7549 - val_accuracy: 0.6915 - val_loss: 0.6178\n",
      "Epoch 2/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 206ms/step - accuracy: 0.6763 - loss: 0.6034 - val_accuracy: 0.7180 - val_loss: 0.5747\n",
      "Epoch 3/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 201ms/step - accuracy: 0.7137 - loss: 0.5581 - val_accuracy: 0.7105 - val_loss: 0.5726\n",
      "Epoch 4/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - accuracy: 0.7430 - loss: 0.5273 - val_accuracy: 0.7075 - val_loss: 0.5654\n",
      "Epoch 5/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - accuracy: 0.7483 - loss: 0.5106 - val_accuracy: 0.6990 - val_loss: 0.6081\n",
      "Epoch 6/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 194ms/step - accuracy: 0.7584 - loss: 0.4907 - val_accuracy: 0.7430 - val_loss: 0.5406\n",
      "Epoch 7/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 180ms/step - accuracy: 0.7755 - loss: 0.4627 - val_accuracy: 0.7225 - val_loss: 0.5753\n",
      "Epoch 8/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 175ms/step - accuracy: 0.7941 - loss: 0.4427 - val_accuracy: 0.7325 - val_loss: 0.5559\n",
      "Epoch 9/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 180ms/step - accuracy: 0.8025 - loss: 0.4237 - val_accuracy: 0.7030 - val_loss: 0.6077\n",
      "Epoch 10/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 178ms/step - accuracy: 0.8098 - loss: 0.4141 - val_accuracy: 0.7385 - val_loss: 0.5647\n",
      "Epoch 11/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 179ms/step - accuracy: 0.8194 - loss: 0.3916 - val_accuracy: 0.7540 - val_loss: 0.5482\n",
      "Epoch 12/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 178ms/step - accuracy: 0.8302 - loss: 0.3671 - val_accuracy: 0.7465 - val_loss: 0.5537\n",
      "Epoch 13/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 177ms/step - accuracy: 0.8300 - loss: 0.3600 - val_accuracy: 0.7450 - val_loss: 0.5760\n",
      "Epoch 14/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 173ms/step - accuracy: 0.8445 - loss: 0.3508 - val_accuracy: 0.7420 - val_loss: 0.5842\n",
      "Epoch 15/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 172ms/step - accuracy: 0.8525 - loss: 0.3288 - val_accuracy: 0.7355 - val_loss: 0.6647\n",
      "Epoch 16/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 174ms/step - accuracy: 0.8540 - loss: 0.3317 - val_accuracy: 0.7455 - val_loss: 0.5966\n",
      "Epoch 17/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 173ms/step - accuracy: 0.8719 - loss: 0.3022 - val_accuracy: 0.7560 - val_loss: 0.6137\n",
      "Epoch 18/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 174ms/step - accuracy: 0.8778 - loss: 0.2883 - val_accuracy: 0.7465 - val_loss: 0.6115\n",
      "Epoch 19/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 171ms/step - accuracy: 0.8897 - loss: 0.2580 - val_accuracy: 0.7470 - val_loss: 0.6493\n",
      "Epoch 20/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 173ms/step - accuracy: 0.8921 - loss: 0.2574 - val_accuracy: 0.7565 - val_loss: 0.5912\n",
      "Epoch 21/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 177ms/step - accuracy: 0.8976 - loss: 0.2445 - val_accuracy: 0.7495 - val_loss: 0.6608\n",
      "Epoch 22/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 178ms/step - accuracy: 0.9036 - loss: 0.2394 - val_accuracy: 0.7520 - val_loss: 0.6935\n",
      "Epoch 23/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 177ms/step - accuracy: 0.9113 - loss: 0.2185 - val_accuracy: 0.7525 - val_loss: 0.7042\n",
      "Epoch 24/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 179ms/step - accuracy: 0.9200 - loss: 0.2051 - val_accuracy: 0.7415 - val_loss: 0.7144\n",
      "Epoch 25/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 183ms/step - accuracy: 0.9186 - loss: 0.1983 - val_accuracy: 0.7500 - val_loss: 0.7652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1aa76b3fb30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707dab4a-266e-48ca-8fc8-eed1314aae3c",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- making a single prediction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c50dfbe-fea1-45fe-b787-473d63000b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6141bd3d-52ac-4eac-8fda-92b910925159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "{'cats': 0, 'dogs': 1}\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image and convert to RGB to ensure 3 channels\n",
    "test_image = Image.open(r\"C:\\Users\\SHRI\\OneDrive\\Desktop\\DataScience_DataFiles\\Image_data\\dataset\\single_prediction\\cat_or_dog_1.jpg\").convert('RGB')\n",
    "\n",
    "# Resize the image to match the model's input size\n",
    "test_image = test_image.resize((64, 64))\n",
    "\n",
    "# Convert image to numpy array\n",
    "test_image = np.array(test_image)\n",
    "\n",
    "# Rescale the pixel values to match the training preprocessing\n",
    "test_image = test_image / 255.0  # Rescale between 0 and 1\n",
    "\n",
    "# Expand dimensions to make it compatible with the model input\n",
    "test_image = np.expand_dims(test_image, axis=0)  # Shape: (1, 64, 64, 3)\n",
    "\n",
    "# Prediction\n",
    "result = classifier.predict(test_image)\n",
    "\n",
    "# Check the class mapping\n",
    "print(training_set.class_indices)\n",
    "\n",
    "# Evaluation\n",
    "if result[0][0] > 0.5:    # Threshold for binary classification\n",
    "    print('Dog')\n",
    "else:\n",
    "    print('Cat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "035f7af8-9af4-4ed4-8da3-3e1b291b2b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (1, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Image Shape:\", test_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cfbaf-0a7d-439f-8efa-31c405207620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c61f8-4adb-4d7c-89c0-85fa2aa56c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dca3d5-53d6-473b-b0ea-1ce0309be016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
