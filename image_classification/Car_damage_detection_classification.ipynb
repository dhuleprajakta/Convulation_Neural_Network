{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dcf0eb14-9ae2-4198-bd9a-ac9a6347c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab251f06-211b-4944-a6f5-bf2148122349",
   "metadata": {},
   "source": [
    "# Busiess problem Understanding\n",
    "\n",
    "-  Problem: Checking car damage by hand is slow and can be wrong.\n",
    "- Goal: Use AI to quickly find car damage from images.\n",
    "- Limits: The model must be fast, accurate, and safe.\n",
    "- Success: Faster claims, fewer mistakes, and less fraud.\n",
    "\n",
    "\n",
    "# Data Unerstanding\n",
    "- Dataset: Contains car images with and without damage.\n",
    "- Features: Image (car photo) and label (damage type).\n",
    "- Distribution: Includes various damage types (scratch, dent, no damage).\n",
    "- Quality: Check for clear images and correct labels.\n",
    "\n",
    "\n",
    "# Data (Image)Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dec40d04-2f42-4c17-be4e-69fdaf120d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88b809e6-d3e6-4fbc-9080-b5ad0880d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2)\n",
    "# maxscalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "27d563af-5d25-4bee-b53c-e7dfd3ba78e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1383 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Resizing\n",
    "training_set = train_datagen.flow_from_directory(r\"C:\\Users\\SHRI\\OneDrive\\Desktop\\DataScience_DataFiles\\Image_data\\data3a\\training\",\n",
    "                                                target_size = (100,100),      # least quality of pixeel in dataset\n",
    "                                                class_mode = 'categorical'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0648c466-110f-4903-8770-285a40b059d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minor': 0, 'moderate': 1, 'severe': 2}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices    # clAss names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "186d82af-b0cc-468f-a4b1-aafad302ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 248 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2)\n",
    "# maxscalling\n",
    "\n",
    "#resizing\n",
    "test_set = test_datagen.flow_from_directory(r\"C:\\Users\\SHRI\\OneDrive\\Desktop\\DataScience_DataFiles\\Image_data\\data3a\\validation\",\n",
    "                                                target_size = (100,100),      # least quality of pixeel in dataset\n",
    "                                                class_mode = 'categorical'\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53139a4f-4b6e-4cba-bfc8-a1964c5fefbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5909f0ea-8542-4ca9-8c72-125c0b894802",
   "metadata": {},
   "source": [
    "# Modelling - Convolution Neural Network\n",
    "\n",
    "- initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d301051-b93c-4570-8636-a8172c39b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "classifier = Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f1527-2621-42f8-b397-55b87d36fcc4",
   "metadata": {},
   "source": [
    "**step 1 - Convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "337619e7-9f83-4800-bc5e-7e8112d237e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "classifier.add(Conv2D(\n",
    "    input_shape=[100, 100, 3],   # Image dimensions: 100 x 100 with 3 color channels (RGB)\n",
    "    filters=32,                 # Number of filters\n",
    "    kernel_size=3,              # Size of each filter (3x3)\n",
    "    activation='relu'           # Correct spelling of the parameter\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab5148cc-9b99-42bb-8985-b388b56d6191",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(\n",
    "    input_shape=[100, 100, 3],   # Image dimensions: 64x64 with 3 color channels (RGB)\n",
    "    filters=64,                 # Number of filters\n",
    "    kernel_size=3,              # Size of each filter (3x3)\n",
    "    activation='relu'           # Correct spelling of the parameter\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4bf554be-b18d-4539-9ef1-af49b451ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(\n",
    "    input_shape=[100, 100, 3],   # Image dimensions: 64x64 with 3 color channels (RGB)\n",
    "    filters=64,                 # Number of filters\n",
    "    kernel_size=3,              # Size of each filter (3x3)\n",
    "    activation='relu'           # Correct spelling of the parameter\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647aa09-58c5-4c42-a4eb-2e56b2dfe8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5557100-a0c2-414c-83df-80b26ccc5d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe4f8c7-bbb9-4b55-b7db-5e4d5c5afe49",
   "metadata": {},
   "source": [
    "**step 2 - Max Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3eb073e6-cb14-4699-99f4-8a88e21c3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b01f76e-4d35-4383-9b8a-2aecb59faa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "68fee3db-a4dc-49a9-b89e-1fad4a70ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fe53c-d521-45e7-9c22-fd0234257d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae89f43-4ff6-44f3-ad88-08b0408b6758",
   "metadata": {},
   "source": [
    "**Step 3 - Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2fd71287-af9b-4e1d-b8f7-edb1690ea6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "classifier.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc94162-1837-4362-b629-1f60706deee5",
   "metadata": {},
   "source": [
    "**Step 4 - Full Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7c2a945-fc7f-43e5-8e6e-230d5e1f2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# 1st hidden layer with 200 neurons\n",
    "classifier.add(Dense(units = 200, activation = 'relu'))\n",
    "\n",
    "# Output layer with 3 neurons for 3 classes\n",
    "classifier.add(Dense(units=3, activation='softmax'))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0391f-1319-4ffd-a730-df255bbf29a2",
   "metadata": {},
   "source": [
    "**Training the CNN model with train data and Testing the model with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd377270-7e65-41e7-ba6b-fa3b3b4f08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',    # For multi-class classification\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4132b9ff-d30f-4363-9d7b-c41a0d9c4442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.3490 - loss: 1.1393 - val_accuracy: 0.4798 - val_loss: 1.0437\n",
      "Epoch 2/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.4246 - loss: 1.0529 - val_accuracy: 0.4395 - val_loss: 1.0659\n",
      "Epoch 3/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.4978 - loss: 1.0032 - val_accuracy: 0.5565 - val_loss: 0.9012\n",
      "Epoch 4/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.5180 - loss: 0.9508 - val_accuracy: 0.5403 - val_loss: 0.9402\n",
      "Epoch 5/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.5880 - loss: 0.9126 - val_accuracy: 0.5323 - val_loss: 0.8952\n",
      "Epoch 6/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.6142 - loss: 0.8754 - val_accuracy: 0.5242 - val_loss: 0.9782\n",
      "Epoch 7/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.5836 - loss: 0.8962 - val_accuracy: 0.5645 - val_loss: 0.9403\n",
      "Epoch 8/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.6645 - loss: 0.7540 - val_accuracy: 0.5806 - val_loss: 0.8874\n",
      "Epoch 9/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.6770 - loss: 0.7417 - val_accuracy: 0.5484 - val_loss: 0.9320\n",
      "Epoch 10/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.6789 - loss: 0.7164 - val_accuracy: 0.5040 - val_loss: 1.0105\n",
      "Epoch 11/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1s/step - accuracy: 0.6885 - loss: 0.7198 - val_accuracy: 0.5363 - val_loss: 0.9409\n",
      "Epoch 12/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1s/step - accuracy: 0.7304 - loss: 0.6240 - val_accuracy: 0.5847 - val_loss: 0.9565\n",
      "Epoch 13/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.7206 - loss: 0.6294 - val_accuracy: 0.5282 - val_loss: 1.0750\n",
      "Epoch 14/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.7119 - loss: 0.6256 - val_accuracy: 0.5161 - val_loss: 1.1903\n",
      "Epoch 15/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.7435 - loss: 0.5876 - val_accuracy: 0.5726 - val_loss: 1.1890\n",
      "Epoch 16/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.8160 - loss: 0.4826 - val_accuracy: 0.5000 - val_loss: 1.2605\n",
      "Epoch 17/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.8160 - loss: 0.4380 - val_accuracy: 0.4839 - val_loss: 1.3387\n",
      "Epoch 18/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.8307 - loss: 0.4216 - val_accuracy: 0.5000 - val_loss: 1.4157\n",
      "Epoch 19/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.8731 - loss: 0.3826 - val_accuracy: 0.5363 - val_loss: 1.3380\n",
      "Epoch 20/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.8425 - loss: 0.4008 - val_accuracy: 0.4758 - val_loss: 1.4788\n",
      "Epoch 21/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.8705 - loss: 0.3189 - val_accuracy: 0.5202 - val_loss: 1.4339\n",
      "Epoch 22/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.8835 - loss: 0.3434 - val_accuracy: 0.5161 - val_loss: 1.4027\n",
      "Epoch 23/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.8852 - loss: 0.2886 - val_accuracy: 0.5524 - val_loss: 1.5533\n",
      "Epoch 24/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.8945 - loss: 0.2858 - val_accuracy: 0.5565 - val_loss: 1.5577\n",
      "Epoch 25/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.8918 - loss: 0.2668 - val_accuracy: 0.5363 - val_loss: 1.6881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x150e2f47770>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x = training_set, \n",
    "               validation_data = test_set, \n",
    "               epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03538f-8bb8-490e-a7f1-71b481280f61",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- making a single prediction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fbe7195-54bb-4841-bacc-d816d1cd07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "376ce69c-1a8c-48fd-b892-6b53bf172c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and convert to RGB\n",
    "\n",
    "# minor\n",
    "# test_image = Image.open(r\"C:\\Users\\SHRI\\OneDrive\\Desktop\\DataScience_DataFiles\\Image_data\\data3a\\sample_picture\\minor.JPEG\").convert('RGB')\n",
    "\n",
    "# moderate\n",
    "#test_image = Image.open(r\"C:\\Users\\SHRI\\OneDrive\\Desktop\\DataScience_DataFiles\\Image_data\\data3a\\sample_picture\\moderatte.jpg\").convert('RGB')\n",
    "\n",
    "#severe\n",
    "test_image = Image.open(r\"C:\\Users\\SHRI\\OneDrive\\Desktop\\DataScience_DataFiles\\Image_data\\data3a\\sample_picture\\severe.JPEG\").convert('RGB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9f60cbb8-cee8-4204-8346-2c6a9c104947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the image to match the model's input size\n",
    "test_image = test_image.resize((100, 100))   # Resize to match model input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2b3c994c-4601-4c29-9eb1-0b8b1b1df45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to numpy array\n",
    "test_image = np.array(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a484871e-4258-42e5-8548-75e75c21ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the pixel values to match the training preprocessing\n",
    "test_image = test_image / 255.0  # Normalize to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2e3c6db5-a056-4c0f-9846-56a2394945e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand dimensions to add batch size\n",
    "test_image = np.expand_dims(test_image, axis=0)  # Shape: (1, 100, 100, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "340b8c25-c0fc-4464-954c-6dcdff74c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02438024, 0.02128647, 0.9543333 ]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "result = classifier.predict(test_image)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2242c08b-ce8c-49dd-9855-ef51e5daaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mapping\n",
    "class_indices = training_set.class_indices\n",
    "class_names = list(class_indices.keys())  # ['Minor', 'Moderate', 'Severe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e670cca7-6295-41bc-b0e6-059155ce9da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the predicted class index\n",
    "predicted_class_index = np.argmax(result)\n",
    "predicted_class_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c818aeb-54af-4ddc-b2ef-42467bdb9b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'severe'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the index to the class name\n",
    "predicted_class = class_names[predicted_class_index]\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "514876a1-c074-48eb-aa40-905c0f05594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: severe\n",
      "Confidence: 95.43%\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Confidence: {result[0][predicted_class_index]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c9bcb-1bd8-45b7-a32b-1b8bef5ccc93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
